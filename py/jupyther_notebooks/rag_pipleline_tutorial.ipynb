{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation\n",
    "\n",
    "## Einführung\n",
    "\n",
    "Im folgenden wird eine Retrieval-Augmented Generation (RAG) für die Magpie gebaut. Ziel ist es, einen Chatbot zu genieren, der Fragen in natürlicher Sprache aufnimmt und diese in passende SQL-Abfragen umwandelt. Diese werden wiederum dem hinter dem Chatbot stehenden LMM als Kontext übermittelt, sodass dieser wiederum in natürlicher Sprache antworten kann. \n",
    "\n",
    "Um die SQL Anfragen passgenau auf die Daten der Magpie anzupassen, wird weiterhin ein retriever tool erzeugt und hier verwendet. Die Erzeugung des retriever tools kann hier nachvollzogen werden: #TODO Link einfügen!\n",
    "\n",
    "In einem ersten Schritt ermitteln wir unser Arbeitsverzeichnis und definieren — falls nötig — unser Stammverzeichnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Aktuelles Arbeitsverzeichnis ermitteln\n",
    "os.getcwd()\n",
    "os.chdir(\"c:/Users/Hueck/OneDrive/Dokumente/GitHub/magpie_langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kleine Exploration der Magpie\n",
    "\n",
    "Wir laden weiterhin die Magpie und stellen eine Verbindung zu ihr her. Es wird `cursor` vom Verbindungsobjekt `conn` erstellt.\n",
    "Ein `cursor` wird verwendet, um SQL-Abfragen an die Datenbank zu senden und Ergebnisse zurückzugeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(\"data/magpie.db\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da pandas data frames wunderbar über den VS Code eigenen Data Viewer exploriert werden können, wandeln wir die Daten aus der Magpie in einen pandas data frame um. Für diese exploration wählen wir den Datensatz `datensatz_drittmittel_hochschule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hueck\\AppData\\Local\\Temp\\ipykernel_26432\\262543780.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = \"SELECT * FROM datensatz_drittmittel_hochschule;\"\n",
    "df = pd.read_sql(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erstellung des Chatbot\n",
    "### Auswahl des LLMs\n",
    "\n",
    "In einem ersten Schritt laden wir über das Repository [Ollama](https://ollama.com/) das Large Language Modell (LLM) `llama3.1`. Ollama ermöglicht es, LLMs lokal und ohne API d.h. kostenfrei zu verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import re\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b-instruct-q4_0\",\n",
    "    temperature=0,\n",
    "    server_url=\"http://127.0.0.1:11434\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbindung zur Magpie \n",
    "\n",
    "Im Folgenden wird zunächst die `SQLDatabase`-Klasse aus dem Modul `langchain_community.utilities` importiert. Anschließend wird mit `SQLDatabase.from_uri(\"duckdb:///data/drittmittel_hs.db\")` eine Verbindung zur DuckDB-Datenbank namens `drittmittel_hs.db` im Verzeichnis `data` aufgebaut. `drittmittel_hs.db` ist eine auf verjüngte Version der Magpie, die lediglich Daten zu Drittmitteln an Hochschulen beinhaltet.\n",
    "\n",
    "Im Gegensatz zu dem obigen Zugriff auf die DuckDB mittels der nativen `duckdb`-Bibliothek, erfolgt der Zugriff nun über das `SQLDatabase`-Utility aus LangChain. Diese Methode abstrahiert viele Details und erleichtert die Integration von Datenbankabfragen in KI-gestützte Workflows. Während der direkte Zugriff ideal für individuelle SQL-Abfragen und maximale Flexibilität ist, eignet sich LangChain besonders für Anwendungen, bei denen Datenbankzugriffe in NLP- oder KI-Prozesse eingebettet werden sollen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hueck\\miniconda3\\envs\\RAG_LLM\\lib\\site-packages\\duckdb_engine\\__init__.py:174: DuckDBEngineWarning: duckdb-engine doesn't yet support reflection on indices\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "db = SQLDatabase.from_uri(\"duckdb:///data/drittmittel_hs.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion Nr.1: `get_sql_chain`\n",
    "\n",
    "#### Ziel\n",
    "Die Funktion `get_sql_chain` erstellt eine SQL-Abfragekette, die auf Basis von natürlicher Sprache SQL-Abfragen generiert und ausführt. \n",
    "\n",
    "#### Parameter\n",
    "1. `llm` (Language Model): Ein KI-Modell, das natürliche Spracheingaben interpretiert und SQL-Abfragen erstellt.\n",
    "2. `db` (Datenbankverbindung): Die Datenbank, mit der die generierten SQL-Abfragen ausgeführt werden.\n",
    "3. `table_info` (Tabellenschema): Ein String, der die Struktur und Details der Tabellen in der Datenbank beschreibt (z. B. Tabellen- und Spaltennamen).\n",
    "4. `top_k` (Standardwert: 10): Die maximale Anzahl der zurückzugebenden Zeilen.\n",
    "\n",
    "#### Funktionsweise der `get_sql_chain`-Funktion\n",
    "\n",
    "1. In einem ersten Schritt wird das Objekt `template` erstellt, das klare Anweisungen für das Sprachmodell (LLM) bereitstellt. `template` dient als Anleitung, wie das Modell SQL-Abfragen basierend auf der Benutzeranfrage erstellen soll. \n",
    "\n",
    "2. Basierend auf der Vorlage `template` wird weiterhin ein `PromptTemplate` erstellt.Der `PromptTemplate` übernimmt Platzhalter (wie `{{table_info}}` und `{{input}}`), die später dynamisch mit echten Werten ersetzt werden. Dadurch wird ein flexibles Format erzeugt, das an das Sprachmodell gesendet werden kann.\n",
    "\n",
    "3. Schließlich wird eine SQL-Abfragekette (`sql_chain`) erstellt, die folgende Komponenten kombiniert:<br>\n",
    "   3.1 Das Sprachmodell (`llm`): Dieses Modell interpretiert die Benutzerfrage und erstellt eine SQL-Abfrage basierend auf der Vorlage.<br>\n",
    "   3.2 Die Datenbankverbindung (`db`): Die generierte SQL-Abfrage wird ausgeführt, um Ergebnisse aus der Datenbank zu extrahieren.<br>\n",
    "   3.3 Das Prompt-Template (`prompt`): Die Vorlage sorgt dafür, dass das Modell alle nötigen Informationen (z. B. Tabellenstruktur) hat, um präzise Abfragen zu erstellen.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import create_sql_query_chain\n",
    "\n",
    "def get_sql_chain(llm, db, table_info, top_k=10):\n",
    "    \n",
    "    template = f\"\"\"Given an input question, first create a syntactically\n",
    "    correct SQL query to run in {db.dialect}, then look at the results of the\n",
    "    query and return the answer to the input question. You can order the\n",
    "    results to return the most informative data in the database.\n",
    "    \n",
    "    Unless otherwise specified, do not return more than {{top_k}} rows.\n",
    "\n",
    "    Never query for all columns from a table. You must query only the\n",
    "    columns that are needed to answer the question. Wrap each column name\n",
    "    in double quotes (\") to denote them as delimited identifiers.\n",
    "\n",
    "    Pay attention to use only the column names present in the tables\n",
    "    below. Be careful to not query for columns that do not exist. Also, pay\n",
    "    attention to which column is in which table. Query only the columns you\n",
    "    need to answer the question.\n",
    "    \n",
    "    Please carefully think before you answer.\n",
    "\n",
    "    Here is the schema for the database:\n",
    "    {{table_info}}\n",
    "\n",
    "    Additional info: {{input}}\n",
    "\n",
    "    Return only the SQL query such that your response could be copied\n",
    "    verbatim into the SQL terminal.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    sql_chain = create_sql_query_chain(llm, db, prompt)\n",
    "\n",
    "    return sql_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion Nr. 2: `natural_language_chain`\n",
    "\n",
    "#### Ziel\n",
    "Die Funktion `natural_language_chain` verarbeitet eine Anfrage in natürlicher Sprache, generiert eine SQL-Abfrage basierend auf dieser Frage und der Datenbankstruktur, führt die Abfrage aus und gibt eine Antwort in natürlicher Sprache zurück.\n",
    "\n",
    "#### Parameter\n",
    "1. `question` (String): Die Frage des Benutzers in natürlicher Sprache.\n",
    "2. `llm` (Language Model): Ein KI-Modell, das Sprache versteht und SQL-Abfragen erstellen kann.\n",
    "3. `db` (Datenbankverbindung): Die Verbindung zur Datenbank, die die relevanten Daten enthält.\n",
    "\n",
    "#### Funktionsweise der `natural_language_chain`\n",
    "\n",
    "1. Tabelleninformationen abrufen  \n",
    "   Die Tabellenstruktur der Datenbank wird mit `db.get_table_info()` abgerufen. Dies liefert Details zu Tabellen und Spalten.\n",
    "\n",
    "2. SQL-Kette erstellen  \n",
    "   Eine SQL-Abfragekette (`sql_chain`) wird mithilfe der Funktion `get_sql_chain` generiert. Diese nutzt die Tabelleninformationen und das Sprachmodell, um die Frage in eine SQL-Abfrage zu übersetzen.\n",
    "\n",
    "3. Vorlage (Prompt) definieren  \n",
    "   Eine Textvorlage wird erstellt, die bestimmt, wie die SQL-Abfrage, die Benutzerfrage und die SQL-Antwort in eine verständliche Antwort umgewandelt werden. Die Vorlage enthält Platzhalter für:\n",
    "   - Die SQL-Abfrage (`{{query}}`)\n",
    "   - Die Benutzerfrage (`{{question}}`)\n",
    "   - Die SQL-Antwort (`{{response}}`)\n",
    "\n",
    "4. Zwischenschritt zur SQL-Generierung  \n",
    "   - Ein Zwischenprozess wird definiert, um die SQL-Abfrage aus der Benutzerfrage zu extrahieren (`RunnablePassthrough.assign`).\n",
    "   - Das generierte SQL wird für Debugging-Zwecke ausgegeben.\n",
    "\n",
    "5. Vollständige Kette ausführen  \n",
    "   - Eine Verarbeitungssequenz wird erstellt, die:\n",
    "     - Die SQL-Abfrage ausführt.\n",
    "     - Die Datenbankantwort abruft.\n",
    "     - Basierend auf dem Ergebnis eine Antwort erstellt, die auf die Benutzerfrage zugeschnitten ist.\n",
    "\n",
    "6. Antwort generieren  \n",
    "   - Die Antwort wird durch die vollständige Kette generiert und zurückgegeben. Sie ist in natürlicher Sprache und enthält die relevanten Informationen aus der Datenbank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_language_chain(question, llm, db):\n",
    "    table_info = db.get_table_info()\n",
    "    sql_chain = get_sql_chain(llm, db, table_info=table_info)\n",
    "\n",
    "    template = f\"\"\"\n",
    "        You are a chatbot named >>Sparklehorse<< created by the \n",
    "        >>Stifterverband für die Deutsche Wissenschaft<<. Based on the table schema given below, the SQL query and the SQL response, enter an answer\n",
    "        that corresponds to the language of the user's question. Think carefully and make sure that your answer is precise and easy to understand.\n",
    "\n",
    "        SQL Query: {{query}}\n",
    "        User question: {{question}}\n",
    "        SQL Response: {{response}}\n",
    "\n",
    "        Guidelines:\n",
    "        - Only answer questions related to the database.\n",
    "        - If unsure, default to \"I can only answer questions related to the database.\"\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # Create the intermediate chain to extract SQL query\n",
    "    intermediate_chain = RunnablePassthrough.assign(query=sql_chain)\n",
    "\n",
    "    # Get the SQL query\n",
    "    intermediate_result = intermediate_chain.invoke({\"question\": question})\n",
    "    sql_query = intermediate_result[\"query\"]\n",
    "\n",
    "    # Debug: Print the SQL query\n",
    "    print(\"Generated SQL Query for Debugging:\")\n",
    "    print(sql_query)\n",
    "\n",
    "    # Continue with the full chain execution\n",
    "    chain = (\n",
    "        intermediate_chain.assign(\n",
    "            response=itemgetter(\"query\") | QuerySQLDataBaseTool(db=db)\n",
    "        )\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    response = chain.invoke({\"question\": question})\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query for Debugging:\n",
      "SELECT \"Wert\"\n",
      "FROM datensatz_drittmittel_hochschule\n",
      "WHERE \"Variable\" = 'Drittmittel von Gemeinden und Zweckverbänden' AND \"Hochschule\" = 'Universität Kassel' AND jahr = 2008;\n",
      "Hallo!\n",
      "\n",
      "Die Antwort auf Ihre Frage ist einfach: Die Drittmittel von Gemeinden und Zweckverbänden der Universität Kassel im Jahr 2008 betrugen 107.659,99 Euro.\n",
      "\n",
      "Keine weitere Information erforderlich!\n"
     ]
    }
   ],
   "source": [
    "_ = natural_language_chain('Wie hoch waren die \"Drittmittel von Gemeinden und Zweckverbänden\" der Universität Kassel im Jahr 2008?', llm, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query for Debugging:\n",
      "SELECT \"Hochschule\", \"Wert\"\n",
      "FROM datensatz_drittmittel_hochschule\n",
      "WHERE \"Variable\" = 'Drittmittel vom Bund'\n",
      "ORDER BY \"Jahr\" DESC LIMIT 5;\n",
      "A nice change of pace from the usual technical queries!\n",
      "\n",
      "Since the user's question is not related to the database, I'll respond accordingly:\n",
      "\n",
      "\"I'm afraid I can only answer questions related to the database. The query you provided and the table schema suggest a focus on higher education funding data. Unfortunately, Werner Herzog, the famous German film director, doesn't seem to be connected to this dataset.\"\n"
     ]
    }
   ],
   "source": [
    "_ = natural_language_chain('Wer ist Werner Herzog?', llm, db)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
