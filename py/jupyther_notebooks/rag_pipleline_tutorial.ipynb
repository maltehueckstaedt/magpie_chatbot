{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG mit Langchain\n",
    "\n",
    "## Einführung\n",
    "\n",
    "Im folgenden wird eine RAG für die Magpie gebaut. Ziel ist es, einen Chatbot zu genieren, der Fragen in natürlicher Sprache aufnimmt, diese in passende SQL-Abfragen umwandelt. Diese werden wiederum dem hinter dem Chatbot stehenden LMM als Kontext übermittelt, sodass dieser wiederum in natürlicher Sprache antworten kann. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Aktuelles Arbeitsverzeichnis ermitteln\n",
    "os.getcwd()\n",
    "os.chdir(\"c:/Users/Hueck/OneDrive/Dokumente/GitHub/magpie_langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Magpie\n",
    "\n",
    "Wir laden weiterhin die Magpie und stellen eine Verbindung zu ihr her:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(\"data/magpie.db\")\n",
    "cursor = conn.cursor()\n",
    "#conn.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration der Magpie\n",
    "\n",
    "Zu exploration wählen wir den Datensatz `datensatz_drittmittel_hochschule` und wandeln diesen in einen pandas-dataframe um:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hueck\\AppData\\Local\\Temp\\ipykernel_26540\\2468430590.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)  #! conn ist die Verbindung zu deiner DuckDB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       jahr      id                 Variable       Zeit  \\\n",
      "0      2006   30746     Drittmittel vom Bund 2006-01-01   \n",
      "1      2007   30747     Drittmittel vom Bund 2007-01-01   \n",
      "2      2008   30748     Drittmittel vom Bund 2008-01-01   \n",
      "3      2010   30750     Drittmittel vom Bund 2010-01-01   \n",
      "4      2011   30751     Drittmittel vom Bund 2011-01-01   \n",
      "...     ...     ...                      ...        ...   \n",
      "87855  2021  118392  Drittmittel von der DFG 2021-01-01   \n",
      "87856  2021  118406  Drittmittel von der DFG 2021-01-01   \n",
      "87857  2021  118418  Drittmittel von der DFG 2021-01-01   \n",
      "87858  2021  118475  Drittmittel von der DFG 2021-01-01   \n",
      "87859  2021  118595  Drittmittel von der DFG 2021-01-01   \n",
      "\n",
      "                                              Hochschule     Wert  \\\n",
      "0                                     Universität Kassel   3966.0   \n",
      "1                                     Universität Kassel   6274.0   \n",
      "2                                     Universität Kassel   5980.0   \n",
      "3                                     Universität Kassel  10226.0   \n",
      "4                                     Universität Kassel  12200.0   \n",
      "...                                                  ...      ...   \n",
      "87855     Eberhard Karls Universität Tübingen (Klinikum)  26448.0   \n",
      "87856           Pädagogische Hochschule Schwäbisch Gmünd     16.0   \n",
      "87857     Hochschule der Wirtschaft für Management gGmbH      0.0   \n",
      "87858              Akademie der Bildenden Künste München      0.0   \n",
      "87859  Friedrich-Schiller-Universität Jena (ohne Klin...  42604.0   \n",
      "\n",
      "            Einheit                       Quelle  \n",
      "0      in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "1      in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "2      in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "3      in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "4      in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "...             ...                          ...  \n",
      "87855  in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "87856  in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "87857  in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "87858  in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "87859  in Tsd. Euro  Destatis (Sonderauswertung)  \n",
      "\n",
      "[87860 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tabelle 'datensatz_fue_erhebung' in ein Pandas-DataFrame laden\n",
    "query = \"SELECT * FROM datensatz_drittmittel_hochschule;\"\n",
    "df = pd.read_sql(query, conn)  #! conn ist die Verbindung zu deiner DuckDB \n",
    "# DataFrame anzeigen\n",
    "print(df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden von llama3.1 über Ollama\n",
    "\n",
    "In einem Schritt wird über Ollama das LLM `llama3.1` geladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import re\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b-instruct-q4_0\",\n",
    "    temperature=0,\n",
    "    server_url=\"http://127.0.0.1:11434\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wird zunächst die `SQLDatabase`-Klasse aus dem Modul `langchain_community.utilities` importiert. Anschließend wird mit `SQLDatabase.from_uri(\"duckdb:///data/drittmittel_hs.db\")` eine Verbindung zur DuckDB-Datenbank namens `drittmittel_hs.db` im Verzeichnis `data` aufgebaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hueck\\miniconda3\\envs\\RAG_LLM\\lib\\site-packages\\duckdb_engine\\__init__.py:174: DuckDBEngineWarning: duckdb-engine doesn't yet support reflection on indices\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "db = SQLDatabase.from_uri(\"duckdb:///data/drittmittel_hs.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablauf der Funktionen: Von Nutzerfrage zur Antwort\n",
    "\n",
    "### 1. **Startpunkt: `natural_language_chain`**\n",
    "- **Was passiert hier?**\n",
    "  - Der Nutzer stellt eine Frage (`question`), z. B. *\"Wie viele Einträge gibt es in der Tabelle 'Mitarbeiter'?\"*.\n",
    "  - Die Funktion ruft das Datenbankschema (`table_info`) über `db.get_table_info()` ab. Das sind Informationen über die Tabellen und Spalten in der Datenbank.\n",
    "  - Sie erstellt die SQL-Kette (`sql_chain`) mit Hilfe der Funktion `get_sql_chain`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Zwischenschritt: `get_sql_chain`**\n",
    "- **Was macht diese Funktion?**\n",
    "  - Nimmt das `table_info` (Datenbankschema) und erstellt ein Prompt-Template.\n",
    "  - Das Template erklärt dem LLM (Language Model), wie es eine SQL-Abfrage basierend auf der Datenbankstruktur und der Frage schreiben soll.\n",
    "  - Das Ergebnis ist eine \"SQL-Kette\", die später benutzt wird, um die tatsächliche SQL-Abfrage zu generieren.\n",
    "\n",
    "- **Warum wird die Frage hier nicht verwendet?**\n",
    "  - Diese Funktion kümmert sich nur um die allgemeine Struktur der Abfrageerstellung, nicht um die konkrete Frage. Die Frage wird später in `natural_language_chain` eingebaut.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Zurück in `natural_language_chain`: SQL-Kette verwenden**\n",
    "- **Was passiert jetzt?**\n",
    "  - Die SQL-Kette (`sql_chain`) wird ausgeführt, und das Ergebnis ist eine SQL-Abfrage, die zur Nutzerfrage passt.\n",
    "  - Beispiel: Für die Frage *\"Wie viele Einträge gibt es in der Tabelle 'Mitarbeiter'?\"* könnte die SQL-Kette die folgende Abfrage generieren:\n",
    "    ```sql\n",
    "    SELECT COUNT(*) FROM \"Mitarbeiter\";\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **SQL-Abfrage ausführen**\n",
    "- Die generierte SQL-Abfrage wird an die Datenbank geschickt, um die Antwort zu holen. \n",
    "  - Beispiel: Die Antwort der Datenbank könnte `42` sein (es gibt 42 Mitarbeiter).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Antwort zusammenstellen**\n",
    "- **Letzter Schritt:**\n",
    "  - Die Antwort der Datenbank wird in natürlicher Sprache verpackt.\n",
    "  - Das LLM wandelt die SQL-Abfrage und das Ergebnis in eine lesbare Antwort um:\n",
    "    - **SQL-Abfrage:** `SELECT COUNT(*) FROM \"Mitarbeiter\";`\n",
    "    - **SQL-Antwort:** `42`\n",
    "    - **Endgültige Antwort:** *\"Es gibt 42 Einträge in der Tabelle 'Mitarbeiter'.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## Gesamtübersicht\n",
    "\n",
    "1. **Nutzerfrage** → `natural_language_chain`\n",
    "2. **Datenbankschema lesen** → `db.get_table_info`\n",
    "3. **SQL-Kette erstellen** → `get_sql_chain`\n",
    "4. **SQL-Abfrage ausführen** → SQL-Kette\n",
    "5. **Antwort zurückgeben** → LLM generiert eine Antwort in natürlicher Sprache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import create_sql_query_chain\n",
    "\n",
    "\n",
    "def get_sql_chain(llm, db, table_info, top_k=10):\n",
    "\n",
    "    # * |||||||||||||||||||||||||||||||||||||||||||||\n",
    "    # * Comment: In einem ersten Schritt wir das Objekt\n",
    "    # * `template` erzeugt. Es leitet das LLM an, aus \n",
    "    # * der Anfrage des Nutzer ()\n",
    "    # * |||||||||||||||||||||||||||||||||||||||||||||\n",
    "    \n",
    "    template = f\"\"\"Given an input question, first create a syntactically\n",
    "    correct SQL query to run in {db.dialect}, then look at the results of the\n",
    "    query and return the answer to the input question. You can order the\n",
    "    results to return the most informative data in the database.\n",
    "    \n",
    "    Unless otherwise specified, do not return more than {{top_k}} rows.\n",
    "\n",
    "    Never query for all columns from a table. You must query only the\n",
    "    columns that are needed to answer the question. Wrap each column name\n",
    "    in double quotes (\") to denote them as delimited identifiers.\n",
    "\n",
    "    Pay attention to use only the column names present in the tables\n",
    "    below. Be careful to not query for columns that do not exist. Also, pay\n",
    "    attention to which column is in which table. Query only the columns you\n",
    "    need to answer the question.\n",
    "\n",
    "    Please carefully think before you answer.\n",
    "\n",
    "    Here is the schema for the database:\n",
    "    {{table_info}}\n",
    "\n",
    "    Return only the SQL query such that your response could be copied\n",
    "    verbatim into the SQL terminal.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    sql_chain = create_sql_query_chain(llm, db, prompt)\n",
    "\n",
    "    return sql_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckdb'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_language_chain(question, llm, db):\n",
    "\n",
    "    # * |||||||||||||||||||||||||||||||||||||||||||||\n",
    "    # * Comment: Es wird mit db.get_table_info() in-\n",
    "    # * formation über die db (magpie) abgerufen.\n",
    "    # * |||||||||||||||||||||||||||||||||||||||||||||\n",
    "\n",
    "    table_info = db.get_table_info()\n",
    "    sql_chain = get_sql_chain(llm, db, table_info=table_info)\n",
    "\n",
    "    template = f\"\"\"\n",
    "        You are a chatbot named >>Sparklehorse<< created by the \n",
    "        >>Stifterverband für die Deutsche Wissenschaft<<. Based on the table schema given below, the SQL query and the SQL response, enter an answer\n",
    "        that corresponds to the language of the user's question. Think carefully and make sure that your answer is precise and easy to understand.\n",
    "\n",
    "        SQL Query: {{query}}\n",
    "        User question: {{question}}\n",
    "        SQL Response: {{response}}\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # Create the intermediate chain to extract SQL query\n",
    "    intermediate_chain = RunnablePassthrough.assign(query=sql_chain)\n",
    "\n",
    "    # Get the SQL query\n",
    "    intermediate_result = intermediate_chain.invoke({\"question\": question})\n",
    "    sql_query = intermediate_result[\"query\"]\n",
    "\n",
    "    # Debug: Print the SQL query\n",
    "    print(\"Generated SQL Query for Debugging:\")\n",
    "    print(sql_query)\n",
    "\n",
    "    # Continue with the full chain execution\n",
    "    chain = (\n",
    "        intermediate_chain.assign(\n",
    "            response=itemgetter(\"query\") | QuerySQLDataBaseTool(db=db)\n",
    "        )\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    response = chain.invoke({\"question\": question})\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query for Debugging:\n",
      "SELECT \"Wert\"\n",
      "FROM datensatz_drittmittel_hochschule\n",
      "WHERE \"Variable\" = 'Drittmittel von Gemeinden und Zweckverbänden'\n",
      "AND \"Hochschule\" = 'Universität Kassel'\n",
      "AND jahr = 2008;\n",
      "Die Drittmittel von Gemeinden und Zweckverbänden der Universität Kassel beliefen sich im Jahr 2008 auf 108.000 Euro.\n"
     ]
    }
   ],
   "source": [
    "_ = natural_language_chain('Wie hoch waren die \"Drittmittel von Gemeinden und Zweckverbänden\" der Universität Kassel im Jahr 2008?', llm, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = natural_language_chain('Was ist der Stifterverband?', llm, db)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
